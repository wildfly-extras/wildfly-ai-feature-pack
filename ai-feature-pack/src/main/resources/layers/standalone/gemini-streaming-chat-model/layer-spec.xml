<?xml version="1.0" encoding="UTF-8"?>
<!--
~ Copyright The WildFly Authors
~ SPDX-License-Identifier: Apache-2.0
-->
<layer-spec xmlns="urn:jboss:galleon:layer-spec:2.0" name="gemini-streaming-chat-model">
    <props>
        <prop name="org.wildfly.rule.configuration" value="https://raw.geminiusercontent.com/wildfly-extras/wildfly-ai-feature-pack/glow-layer-doc/gemini-streaming-chat-model/env.yaml"/>
        <prop name="org.wildfly.rule.add-on-depends-on" value="only:ai"/>
        <prop name="org.wildfly.rule.add-on" value="ai-llm,gemini-chat-model"/>
        <prop name="org.wildfly.rule.add-on-description" value="Use Github Models for LLM interactions"/>
        <prop name="org.wildfly.rule.annotated.type" value="dev.langchain4j.model.chat.StreamingChatModel,jakarta.inject.Named[value=streaming-gemini]"/>
        <prop name="org.wildfly.rule.annotation.field.value" value="io.smallrye.llm.spi.RegisterAIService,streamingChatModelName=streaming-gemini"/>
    </props>
    <dependencies>
        <layer name="ai"/>
    </dependencies>
    <packages>
        <package name="dev.langchain4j.gemini"/>
    </packages>
    <feature spec="subsystem.ai">
        <feature spec="subsystem.ai.gemini-chat-model">
            <param name="gemini-chat-model" value="streaming-gemini"/>
            <param name="api-key" value="${env.GEMINI_API_KEY:YOUR_KEY_VALUE}"/>
            <param name="model-name" value="${org.wildfly.ai.gemini.chat.model.name,env.GEMINI_CHAT_MODEL_NAME:gemini-1.5-flash}"/>
            <param name="log-requests-responses" value="${org.wildfly.ai.gemini.chat.log,env.GEMINI_CHAT_LOG:true}"/>
            <param name="streaming" value="true"/>
        </feature>
    </feature>
</layer-spec>
