#
# Copyright The WildFly Authors
# SPDX-License-Identifier: Apache-2.0
#

ai=The AI subsystem
ai.add=This operation adds the AI subsystem
ai.remove=This operation removes the AI subsystem

ai.chat-memory=Chat Memory.
ai.chat-memory.add=This operation adds a chat memory.
ai.chat-memory.remove=This operation removes a chat memory.
ai.chat-memory.size=The size of the chat memory.
ai.chat-memory.use-http-session=To use the HTTP session id as chat memory id.
ai.chat-memory.type=The type of chat memory.

ai.deployment.chat-model=Chat model used by the current deployment.
ai.deployment.chat-model.streaming=Indicate if the ChatModel is streaming tokens or not.

ai.embedding-store-content-retriever=A ContentRetriever that retrieves content from an embedding store.
ai.embedding-store-content-retriever.add=This operation adds an embedding store content retriever.
ai.embedding-store-content-retriever.embedding-model=Embedding model used to compute embeddings.
ai.embedding-store-content-retriever.embedding-store=Store were the contents and embeddings are retrieved from.
ai.embedding-store-content-retriever.filter=Filter to filter the retrieved contents
ai.embedding-store-content-retriever.min-score=The minimum relevance score for the returned contents.Contents scoring below this score are excluded from the results.
ai.embedding-store-content-retriever.max-results=The maximum number of contents to retrieve.
ai.embedding-store-content-retriever.remove=This operation removes an embedding store content retriever.

ai.gemini-chat-model=Chat model for Google Gemini
ai.gemini-chat-model.add=This operation adds a Google Gemini chat model.
ai.gemini-chat-model.remove=This operation removes a Google Gemini chat model.
ai.gemini-chat-model.allowed-code-execution=Enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output.
ai.gemini-chat-model.api-key=API key to authenticate to a Google Gemini chat model.
ai.gemini-chat-model.civic-integrity=Defines the threshold to filter out civic integrity content.
ai.gemini-chat-model.connect-timeout=Timeout for the Google Gemini chat model.
ai.gemini-chat-model.dangerous-content=Defines the threshold to filter out dangerous content.
ai.gemini-chat-model.enable-enhanced-civic-answers= Enables enhanced civic answers. It may not be available for all models.
ai.gemini-chat-model.frequency-penalty=Positive values penalize tokens that repeatedly appear in the generated text, decreasing the probability of repeating content.
ai.gemini-chat-model.harassment=Defines the threshold to filter out harassment content.
ai.gemini-chat-model.hate-speech=Defines the threshold to filter out hate speech content.
ai.gemini-chat-model.include-code-execution-output=Enables providing the code and the result in the output.
ai.gemini-chat-model.include-thoughts=Enable the return of the thought summaries of the model.
ai.gemini-chat-model.log-requests=Enabling the tracing of requests to Google Gemini
ai.gemini-chat-model.log-responses=Enabling the tracing of responses to Google Gemini
ai.gemini-chat-model.max-output-token=The number of token returned by the Google Gemini chat model.
ai.gemini-chat-model.model-name=Name of the model served by Google Gemini.
ai.gemini-chat-model.presence-penalty=Presence penalty applied to the next token's logprobs if the token has already been seen in the response. This penalty is binary on/off and not dependant on the number of times the token is used (after the first). Use frequency-penalty for a penalty that increases with each use.
ai.gemini-chat-model.response-format=The format of the response from Google Gemini.
ai.gemini-chat-model.return-log-probs=Returns the log probabilities of the top candidate tokens at each generation step.
 ai.gemini-chat-model.response-log-probs=If true, returns the log probabilities of the tokens that were chosen by the model at each step.
ai.gemini-chat-model.return-thinking=Controls whether to return thinking/reasoning text (if available).
ai.gemini-chat-model.seed=When seed is fixed to a specific value, the model makes a best effort to provide the same response for repeated requests. Deterministic output isn't guaranteed.
ai.gemini-chat-model.sexually-explicit=Defines the threshold to filter out sexually explicit content.
ai.gemini-chat-model.stop-sequences=List of stop sequences to tell the model to stop generating content.
ai.gemini-chat-model.streaming=Whether to create a token streaming chat language model or not.
ai.gemini-chat-model.temperature=Temperature of the Google Gemini chat model.
ai.gemini-chat-model.thinking-budget= Parameter to guide the model on the number of thinking tokens to use when generating a response.
ai.gemini-chat-model.top-k=TopK of the Google Gemini chat model.
ai.gemini-chat-model.top-p=TopP of the Google Gemini chat model.
ai.gemini-chat-model.chat=Simple operation to send a user message to the LLM.
ai.gemini-chat-model.chat.user-message=The user message sent to the LLM to test it.

ai.github-chat-model=Chat model for GitHub Models
ai.github-chat-model.add=This operation adds a GitHub Models chat model.
ai.github-chat-model.remove=This operation removes a GitHub Models chat model.
ai.github-chat-model.api-key=API key to authenticate to a GitHub Models chat model.
ai.github-chat-model.connect-timeout=Timeout for the GitHub Models chat model.
ai.github-chat-model.custom-headers=Headers added to the requests to GitHub Models.
ai.github-chat-model.endpoint=Endpoint to connect to an GitHub Models chat model.
ai.github-chat-model.frequency-penalty=Frequency penalty of the GitHub Models chat model.
ai.github-chat-model.log-requests-responses=Enabling the tracing of requests and responses to GitHub Models.
ai.github-chat-model.max-retries=The number of token retries when sending a request to GitHub Models.
ai.github-chat-model.max-token=The number of token returned by the GitHub Models chat model.
ai.github-chat-model.model-name=Name of the model served by GitHub Models.
ai.github-chat-model.organization-id=Name of the organization id served by GitHub Models.
ai.github-chat-model.presence-penalty=Presence penalty of the GitHub Models chat model.
ai.github-chat-model.response-format=The format of the response from GitHub Models.
ai.github-chat-model.seed=Seed of the GitHub Models chat model.
ai.github-chat-model.service-version=Version of GitHub Models to use.
ai.github-chat-model.streaming=Whether to create a token streaming chat language model or not.
ai.github-chat-model.temperature=Temperature of the GitHub Models chat model.
ai.github-chat-model.top-p=Top P of the GitHub Models chat model.
ai.github-chat-model.user-agent-suffix=Suffix to the user-agent.
ai.github-chat-model.chat=Simple operation to send a user message to the LLM.
ai.github-chat-model.chat.user-message=The user message sent to the LLM to test it.

ai.in-memory-embedding-model=Embedding model for AI
ai.in-memory-embedding-model.add=This operation adds an embedding model
ai.in-memory-embedding-model.remove=This operation removes an embedding model
ai.in-memory-embedding-model.module=The module to load the embedding model from
ai.in-memory-embedding-model.embedding-class=The class from which the embedding model will be instantiated

ai.in-memory-embedding-store=In memory embedding store for AI.
ai.in-memory-embedding-store.add=This operation adds an in memory embedding store.
ai.in-memory-embedding-store.remove=This operation removes an in memory embedding store.
ai.in-memory-embedding-store.path=The actual filesystem path to be loaded as content of the in-memory embedding store. Treated as an absolute path, unless the 'relative-to' attribute is specified, in which case the value is treated as relative to that path.
ai.in-memory-embedding-store.relative-to=Reference to a filesystem path defined in the "paths" section of the server configuration.

ai.mcp-client-sse=MCP SSE client.
ai.mcp-client-sse.add=This operation adds an MCP SSE client.
ai.mcp-client-sse.remove=This operation removes MCP SSE client.
ai.mcp-client-sse.connect-timeout=Timeout for the MCP SSE client.
ai.mcp-client-sse.log-requests=Enabling the tracing of requests of the MCP SSE client.
ai.mcp-client-sse.log-responses=Enabling the tracing of responses of the MCP SSE client.
ai.mcp-client-sse.ssl-enabled=True if the connection to the MCP server is https or not.
ai.mcp-client-sse.socket-binding=The outbound socket binding to connect to the MCP server.
ai.mcp-client-sse.sse-path=The URL path to append to access the MCP server SSE endpoint.

ai.mcp-client-stdio=MCP Stdio Client.
ai.mcp-client-stdio.add=This operation adds an MCP Stdio Client.
ai.mcp-client-stdio.remove=This operation removes MCP Stdio Client.
ai.mcp-client-stdio.cmd=The command to use to start the MCP Stdio server.
ai.mcp-client-stdio.args=The arguments to pass to the command to use to start the MCP Stdio server.

ai.mcp-tool-provider=Tool provider using MCP protocol.
ai.mcp-tool-provider.add=This operation adds an MCP Tool provider.
ai.mcp-tool-provider.remove=This operation removes an MCP Tool provider.
ai.mcp-tool-provider.mcp-clients=List of MCP clients used to provide tools.
ai.mcp-tool-provider.fail-if-one-server-fails=If this is true, then the tool provider will throw an exception if it fails to list tools from any of the servers otherwise the tool provider will ignore the error and continue with the next server.

ai.mistral-ai-chat-model=Chat model for Mitral AI
ai.mistral-ai-chat-model.add=This operation adds a Mitral AI chat model.
ai.mistral-ai-chat-model.remove=This operation removes a Mitral AI chat model.
ai.mistral-ai-chat-model.api-key=API key to authenticate to a Mitral AI chat model.
ai.mistral-ai-chat-model.base-url=Endpoint to connect to a Mitral AI chat model.
ai.mistral-ai-chat-model.connect-timeout=Timeout for the Mitral AI chat model.
ai.mistral-ai-chat-model.log-requests=Enabling the tracing of requests going to Mitral AI.
ai.mistral-ai-chat-model.log-responses=Enabling the tracing of responses from Mitral AI.
ai.mistral-ai-chat-model.max-token=The number of token returned by the OpenAI chat model.
ai.mistral-ai-chat-model.model-name=Name of the model served by Mitral AI.
ai.mistral-ai-chat-model.response-format=The format of the response from Mitral AI.
ai.mistral-ai-chat-model.random-seed=Seed of the Mitral AI chat model.
ai.mistral-ai-chat-model.safe-prompt=Whether to inject a safety prompt before all conversations.
ai.mistral-ai-chat-model.streaming=Whether to create a token streaming chat language model or not.
ai.mistral-ai-chat-model.temperature=Temperature of the Mitral AI chat model.
ai.mistral-ai-chat-model.top-p=Top P of the Mitral AI chat model.
ai.mistral-ai-chat-model.chat=Simple operation to send a user message to the LLM.
ai.mistral-ai-chat-model.chat.user-message=The user message sent to the LLM to test it

ai.neo4j-content-retriever=Neo4J content retriever.
ai.neo4j-content-retriever.add=This operation adds a Neo4J content retriever.
ai.neo4j-content-retriever.remove=This operation removes a Neo4J content retriever.
ai.neo4j-content-retriever.bolt-url=The Bolt URL to connect to the Neo4J server.
ai.neo4j-content-retriever.chat-language-model=The chat language model name of the model used to create cypher queries.
ai.neo4j-content-retriever.credential-reference=Credential (from Credential Store) to authenticate to Neo4J.
ai.neo4j-content-retriever.credential-reference.store=The name of the credential store holding the alias to credential
ai.neo4j-content-retriever.credential-reference.type=The type of credential this reference is denoting
ai.neo4j-content-retriever.credential-reference.alias=The alias which denotes stored secret or credential in the store
ai.neo4j-content-retriever.credential-reference.clear-text=Secret specified using clear text (check credential store way of supplying credential/secrets to services)
ai.neo4j-content-retriever.prompt-template=The prompt template used by the chat language model to create cypher queries.
ai.neo4j-content-retriever.username=The username to connect to Neo4J.

ai.neo4j-embedding-store=Neo4J embedding store for AI.
ai.neo4j-embedding-store.add=This operation adds a Neo4J embedding store.
ai.neo4j-embedding-store.remove=This operation removes a Neo4J embedding store.
ai.neo4j-embedding-store.bolt-url=The Bolt URL to connect to the Neo4J server.
ai.neo4j-embedding-store.credential-reference=Credential (from Credential Store) to authenticate to Neo4J.
ai.neo4j-embedding-store.credential-reference.store=The name of the credential store holding the alias to credential
ai.neo4j-embedding-store.credential-reference.type=The type of credential this reference is denoting
ai.neo4j-embedding-store.credential-reference.alias=The alias which denotes stored secret or credential in the store
ai.neo4j-embedding-store.credential-reference.clear-text=Secret specified using clear text (check credential store way of supplying credential/secrets to services)
ai.neo4j-embedding-store.database-name=The Neo4J database name.
ai.neo4j-embedding-store.dimension=The dimension of the embedding.
ai.neo4j-embedding-store.embedding-property=The name of the property to store the embedding.
ai.neo4j-embedding-store.id-property=The optional id property name.
ai.neo4j-embedding-store.index-name=The optional index name.
ai.neo4j-embedding-store.label=The optional label name.
ai.neo4j-embedding-store.metadata-prefix=The optional metadata prefix.
ai.neo4j-embedding-store.retrieval-query=The optional retrieval query.
ai.neo4j-embedding-store.text-property=The optional textProperty property name
ai.neo4j-embedding-store.username=The username to connect to Neo4J.

ai.ollama-chat-model=Chat model for AI for Ollama.
ai.ollama-chat-model.add=This operation adds a chat model
ai.ollama-chat-model.remove=This operation removes a chat model
ai.ollama-chat-model.base-url=Endpoint to connect to an Ollama chat model.
ai.ollama-chat-model.connect-timeout=Timeout for the Ollama chat model.
ai.ollama-chat-model.log-requests=Enabling the tracing of requests going to Ollama.
ai.ollama-chat-model.log-responses=Enabling the tracing of responses from Ollama.
ai.ollama-chat-model.max-retries==The maximum number of retries for API requests.
ai.ollama-chat-model.model-name=Name of the chat model served by Ollama.
ai.ollama-chat-model.response-format=The format of the response from Ollama.
ai.ollama-chat-model.streaming=Whether to create a token streaming chat language model or not.
ai.ollama-chat-model.temperature=Temperature of the Ollama chat model.
ai.ollama-chat-model.chat=Simple operation to send a user message to the LLM.
ai.ollama-chat-model.chat.user-message=The user message sent to the LLM to test it.

ai.ollama-embedding-model=Embedding model for AI for Ollama.
ai.ollama-embedding-model.add=This operation adds a embedding model
ai.ollama-embedding-model.remove=This operation removes a embedding model
ai.ollama-embedding-model.base-url=Endpoint to connect to an Ollama embedding model.
ai.ollama-embedding-model.connect-timeout=Timeout for the Ollama embedding model.
ai.ollama-embedding-model.log-requests=Enabling the tracing of requests going to Ollama.
ai.ollama-embedding-model.log-responses=Enabling the tracing of responses from Ollama.
ai.ollama-embedding-model.model-name=Name of the embedding model served by Ollama.

ai.openai-chat-model=Chat model for OpenAI
ai.openai-chat-model.add=This operation adds an OpenAI chat model.
ai.openai-chat-model.remove=This operation removes an OpenAI chat model.
ai.openai-chat-model.api-key=API key to authenticate to an OpenAI chat model.
ai.openai-chat-model.base-url=Endpoint to connect to an OpenAI chat model.
ai.openai-chat-model.connect-timeout=Timeout for the OpenAI chat model.
ai.openai-chat-model.frequency-penalty=Frequency penalty of the OpenAI chat model.
ai.openai-chat-model.log-requests=Enabling the tracing of requests going to OpenAI.
ai.openai-chat-model.log-responses=Enabling the tracing of responses from OpenAI.
ai.openai-chat-model.max-token=The number of token returned by the OpenAI chat model.
ai.openai-chat-model.model-name=Name of the model served by OpenAI.
ai.openai-chat-model.organization-id=Name of the organization id served by OpenAI.
ai.openai-chat-model.presence-penalty=Presence penalty of the OpenAI chat model.
ai.openai-chat-model.response-format=The format of the response from OpenAI.
ai.openai-chat-model.seed=Seed of the OpenAI chat model.
ai.openai-chat-model.streaming=Whether to create a token streaming chat language model or not.
ai.openai-chat-model.temperature=Temperature of the OpenAI chat model.
ai.openai-chat-model.top-p=Top P of the OpenAI chat model.
ai.openai-chat-model.chat=Simple operation to send a user message to the LLM.
ai.openai-chat-model.chat.user-message=The user message sent to the LLM to test it.

ai.weaviate-embedding-store=Weaviate embedding store for AI.
ai.weaviate-embedding-store.add=This operation adds a weaviate embedding store.
ai.weaviate-embedding-store.avoid-dups=If true the object id is a hashed ID based on provided text segment else a random ID will be generated.
ai.weaviate-embedding-store.remove=This operation removes a weaviate embedding store.
ai.weaviate-embedding-store.consistency-level=How the consistency is tuned when writing into weaviate embedding store.
ai.weaviate-embedding-store.metadata=The list of metadata keys to store with an embedding content.
ai.weaviate-embedding-store.object-class=The object class under which the embeddings are stored.
ai.weaviate-embedding-store.ssl-enabled=True if the connection to the Weaviate store is https or not.
ai.weaviate-embedding-store.socket-binding=The outbound socket binding to connect to the Weaviate store.

ai.web-search-content-retriever=Content retriever using web search results.
ai.web-search-content-retriever.max-results=The maximum number of results.
ai.web-search-content-retriever.google=The custom Google web search engine configuration.
ai.web-search-content-retriever.google.api-key=The Google Search API key for accessing the Google Custom Search API.
ai.web-search-content-retriever.google.connect-timeout=Timeout for the custom Google search.
ai.web-search-content-retriever.google.custom-search-id=The Custom Search ID parameter for search the entire web
ai.web-search-content-retriever.google.include-images=If it is true then include public images relevant to the query. This can add more latency to the search.
ai.web-search-content-retriever.google.log-requests=Whether to log API requests.
ai.web-search-content-retriever.google.log-responses=Whether to log API responses.
ai.web-search-content-retriever.google.max-retries=The maximum number of retries for API requests.
ai.web-search-content-retriever.google.site-restrict=If your Search Engine is restricted to only searching specific sites, you can set this parameter to true.
ai.web-search-content-retriever.tavily=The Tavily Search engine configuration.
ai.web-search-content-retriever.tavily.api-key=Your Tavily API key.
ai.web-search-content-retriever.tavily.base-url=The base url to access Tavily API.
ai.web-search-content-retriever.tavily.connect-timeout=Timeout for the Tavily API to respond.
ai.web-search-content-retriever.tavily.exclude-domains=A list of domains to specifically exclude from the search results.
ai.web-search-content-retriever.tavily.include-answer=Include answers in the search results.
ai.web-search-content-retriever.tavily.include-domains=A list of domains to specifically include in the search results.
ai.web-search-content-retriever.tavily.include-raw-content=Include raw content in the search results.
ai.web-search-content-retriever.tavily.search-depth=The depth of the search. It can be basic or advanced. Default is basic for quick results and advanced for indepth high quality results but longer response time. Advanced calls equals 2 requests.
