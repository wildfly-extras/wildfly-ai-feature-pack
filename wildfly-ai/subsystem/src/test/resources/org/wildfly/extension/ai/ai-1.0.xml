<!--
~ Copyright The WildFly Authors
~ SPDX-License-Identifier: Apache-2.0
-->

<subsystem xmlns="urn:jboss:domain:ai:1.0">
    <chat-language-models>
        
        <gemini-chat-model name="gemini" api-key="${env.GEMINI_API_KEY:key}" include-code-execution-output="true" harassment="${org.wildfly.ai.gemini.harassment,env.GEMINI_HARASSMENT:BLOCK_LOW_AND_ABOVE}" log-requests="${org.wildfly.ai.gemini.chat.log,env.GEMINI_CHAT_LOG:true}" log-responses="${org.wildfly.ai.gemini.chat.log,env.GEMINI_CHAT_LOG:true}" model-name="${org.wildfly.ai.gemini.chat.model.name,env.GEMINI_CHAT_MODEL_NAME:gpt-4o-mini}" streaming="false" temperature="${org.wildfly.ai.gemini.chat.temperature,env.GEMINI_CHAT_TEMPERATURE:0.9}"/>
        <github-chat-model name="github" api-key="${env.GITHUB_API_KEY:key}" endpoint="${org.wildfly.ai.github.chat.url,env.GITHUB_CHAT_URL:https://models.inference.ai.azure.com}" log-requests-responses="${org.wildfly.ai.github.chat.log,env.GITHUB_CHAT_LOG:true}" model-name="${org.wildfly.ai.github.chat.model.name,env.GITHUB_CHAT_MODEL_NAME:gpt-4o-mini}" streaming="false" temperature="${org.wildfly.ai.github.chat.temperature,env.GITHUB_CHAT_TEMPERATURE:0.9}"/>
        <ollama-chat-model name="ollama" base-url="${org.wildfly.ai.ollama.chat.url,env.OLLAMA_CHAT_URL:http://127.0.0.1:11434}" log-requests="${org.wildfly.ai.ollama.chat.log.request,env.OLLAMA_CHAT_LOG_REQUEST:true}" log-responses="${org.wildfly.ai.ollama.chat.log.response,env.OLLAMA_CHAT_LOG_RESPONSE:true}" model-name="${org.wildfly.ai.ollama.chat.model.name,env.OLLAMA_CHAT_MODEL_NAME:llama3.1:8b}" temperature="${org.wildfly.ai.ollama.chat.temperature,env.OLLAMA_CHAT_TEMPERATURE:0.9}"/>
        <ollama-chat-model name="streaming-ollama" base-url="${org.wildfly.ai.ollama.chat.url,env.OLLAMA_CHAT_URL:http://127.0.0.1:11434}" log-requests="${org.wildfly.ai.ollama.chat.log.request,env.OLLAMA_CHAT_LOG_REQUEST:true}" log-responses="${org.wildfly.ai.ollama.chat.log.response,env.OLLAMA_CHAT_LOG_RESPONSE:true}" model-name="${org.wildfly.ai.ollama.chat.model.name,env.OLLAMA_CHAT_MODEL_NAME:llama3.1:8b}" streaming="true" temperature="${org.wildfly.ai.ollama.chat.temperature,env.OLLAMA_CHAT_TEMPERATURE:0.9}"/>
        <openai-chat-model name="chat" base-url="http://langchain4j.dev/demo/openai/v1" api-key="demo" model-name="gpt-4" streaming="false"/>
        <openai-chat-model name="mychat" api-key="${env.GROQ_API_KEY}" base-url="https://api.groq.com/openai/v1" model-name="llama3-8b-8192" streaming="false"/>
        <mistral-ai-chat-model name="mistral" api-key="demo" base-url="https://api.mistral.ai/v1" model-name="mistral-small-latest" streaming="false"/>
    </chat-language-models>
    <embedding-models>
        <in-memory-embedding-model name="myembedding" module="dev.langchain4j.embeddings.all-minilm-l6-v2" embedding-class="dev.langchain4j.model.embedding.AllMiniLmL6V2EmbeddingModel"/>
    </embedding-models>
    <embedding-stores>
        <in-memory-embedding-store name="mystore" path="/home/ehugonne/dev/AI/crawler/crawler/wildfly-admin-embeddings.json"/>
        <neo4j-embedding-store name="myNeo4jstore" bolt-url="bolt://localhost:7687" username="neo4j" dimension="384">
            <credential-reference clear-text="passwordOut!"/>
        </neo4j-embedding-store>
        <weaviate-embedding-store name="myWeaviatestore" socket-binding="weaviate" ssl-enabled="false" object-class="Simple" metadata="url language parent_url file_name file_path title subtitle"/>
    </embedding-stores>
    <content-retrievers>
        <embedding-store-content-retriever name="myretriever" embedding-store="mystore" embedding-model="myembedding"/>
        <neo4j-content-retriever name="myNeo4jstore" bolt-url="bolt://localhost:7687" username="neo4j" chat-language-model="ollama" prompt-template="template">
            <credential-reference clear-text="passwordOut!"/>
        </neo4j-content-retriever>
        <web-search-content-retriever name="google-test">
            <google api-key="key" connect-timeout="10000" custom-search-id="id" log-requests="true" log-responses="true"/>
        </web-search-content-retriever>
        <web-search-content-retriever name="tavily-test">
            <tavily api-key="key" base-url="https://api.tavily.com" connect-timeout="20000" exclude-domains="example.org" include-answer="true" include-domains="example.com" include-raw-content="true" search-depth="basic"/>
        </web-search-content-retriever>
    </content-retrievers>
    <chat-memories>
        <chat-memory name="chat-memory" size="${org.wildfly.ai.chat-memory.size,env.CHAT_MEMORY_SIZE:5}" type="${org.wildfly.ai.chat-memory.type,env.CHAT_MEMORY_TYPE:MESSAGE}" use-http-session="${org.wildfly.ai.chat-memory.session,env.CHAT_MEMORY_SESSION:true}"/>
    </chat-memories>
    <mcp>
        <mcp-tool-provider name="mcp-tools" mcp-clients="mcp-sse" fail-if-one-server-fails="true"/>
        <mcp-client-sse name="mcp-sse" socket-binding="mcp" ssl-enabled="false" log-requests="${org.wildfly.ai.mcp.sse.log.request:true}" log-responses="${org.wildfly.ai.mcp.sse.log.response:true}" sse-path="/sse" connect-timeout="20000"/>
    </mcp>
</subsystem>